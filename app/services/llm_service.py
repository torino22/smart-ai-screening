from openai import AsyncOpenAI

from app.config.settings import (
    OPENAI_API_KEY,
    OPENAI_BASE_URL,
    ACTIVE_MODEL
)
from app.pydantics.schemas import GenericServiceResponse
from app.templates.chat_prompt_template import dynamic_prompt
from app.utils.logger import log_info, log_error


CLIENT = AsyncOpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)


class LLMService:

    @staticmethod
    async def chat(query, top_k_matches) -> GenericServiceResponse:
        try:
            prompt = dynamic_prompt(query, top_k_matches)

            response = await CLIENT.chat.completions.create(
                model=ACTIVE_MODEL,
                messages=[
                    {
                        "role": "system",
                        "content": prompt
                    }
                ],
                response_format={"type": "text"},
                max_tokens=1500,
                temperature=0.2
            )

            llm_text = response.choices[0].message.content.strip()
            log_info("Response generated by the LLM: \n"
                     f"Query: {query} \n"
                     f"LLM response: {llm_text}")

            return GenericServiceResponse(
                   result=llm_text
                   )

        except Exception as e:
            log_error(f"Error while generating response for this query: {query}. "
                     f"Error: {str(e)}")
            return GenericServiceResponse(
                success=False,
                error=str(e)
            )